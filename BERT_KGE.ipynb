{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_KGE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3ELy-uPDvRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZcyYMdeEI0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pickle\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oYsV4H8fCpZ-",
        "colab": {}
      },
      "source": [
        "# Checking if cuda GPU available\n",
        "if torch.cuda.is_available():    \n",
        "    # Using the cuda device    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('Cuda is available. Using the device', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('GPU is not available,so using the CPU ')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me5UOkbKFG0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class defines the Neural Network using BERT for text classification\n",
        "class BERTwithoutKGE(nn.Module):\n",
        "    def __init__(self, num_labels, bert_out_dim=768, nn_dim=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = {\n",
        "            'num_labels': num_labels,\n",
        "            'bert_out_dim': bert_out_dim,\n",
        "            'nn_dim': nn_dim,\n",
        "            'dropout': dropout,\n",
        "        }\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(bert_out_dim, int(bert_out_dim/2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(bert_out_dim/2), nn_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nn_dim, num_labels)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, token_ids, attention_masks):\n",
        "        _, bert_output = self.bert(token_ids, attention_mask=attention_masks, output_all_encoded_layers=False)\n",
        "        drop_output = self.dropout(bert_output)\n",
        "        nn_output = self.nn(drop_output)\n",
        "        prob = self.softmax(nn_output)\n",
        "\n",
        "        return prob\n",
        "\n",
        "# This class defines the KGE Model 1 using BERT for text classification\n",
        "class BERTwithKGE1(nn.Module):\n",
        "    def __init__(self, num_labels, bert_out_dim=768, nn_dim=100, dropout=0.1, kg_dim=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = {\n",
        "            'num_labels': num_labels,\n",
        "            'bert_out_dim': bert_out_dim,\n",
        "            'nn_dim': nn_dim,\n",
        "            'dropout': dropout,\n",
        "        }\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(bert_out_dim + kg_dim, int(bert_out_dim/2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(bert_out_dim/2), nn_dim),\n",
        "            nn.ReLU(),            \n",
        "            nn.Linear(nn_dim, num_labels)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, token_ids, attention_masks, kge):\n",
        "        _, bert_output = self.bert(token_ids, attention_mask=attention_masks, output_all_encoded_layers=False)\n",
        "        drop_output = self.dropout(bert_output)\n",
        "        concat = torch.cat((drop_output, kge), dim=1)\n",
        "        nn_output = self.nn(concat)\n",
        "        prob = self.softmax(nn_output)\n",
        "\n",
        "        return prob\n",
        "\n",
        "# This class defines the KGE Model 2 using BERT for text classification\n",
        "class BERTwithKGE2(nn.Module):\n",
        "    def __init__(self, num_labels, bert_out_dim=768, nn_dim=100, dropout=0.1, kg_dim=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = {\n",
        "            'num_labels': num_labels,\n",
        "            'bert_out_dim': bert_out_dim,\n",
        "            'nn_dim': nn_dim,\n",
        "            'dropout': dropout,\n",
        "            'kg_dim' : kg_dim\n",
        "        }\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bert_pooled = nn.Sequential(\n",
        "            nn.Linear(bert_out_dim, int(bert_out_dim/2)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(int(bert_out_dim/2) + kg_dim, nn_dim),\n",
        "            nn.ReLU(),            \n",
        "            nn.Linear(nn_dim, num_labels)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, token_ids, attention_masks, kge):\n",
        "        _, bert_output = self.bert(token_ids, attention_mask=attention_masks, output_all_encoded_layers=False)\n",
        "        drop_output = self.dropout(bert_output)\n",
        "        bert_pool_output = self.bert_pooled(drop_output)\n",
        "        concat = torch.cat((bert_pool_output, kge), dim=1)\n",
        "        nn_output = self.nn(concat)\n",
        "        prob = self.softmax(nn_output)\n",
        "\n",
        "        return prob\n",
        "\n",
        "# This class defines the KGE Model 2 using BERT for text classification\n",
        "class BERTwithKGE3(nn.Module):\n",
        "    def __init__(self, num_labels, bert_out_dim=768, nn_dim=100, dropout=0.1, kg_dim=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = {\n",
        "            'num_labels': num_labels,\n",
        "            'bert_out_dim': bert_out_dim,\n",
        "            'nn_dim': nn_dim,\n",
        "            'dropout': dropout,\n",
        "            'kg_dim' : kg_dim\n",
        "        }\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bert_pooled = nn.Sequential(\n",
        "            nn.Linear(bert_out_dim, nn_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(nn_dim + kg_dim, nn_dim),\n",
        "            nn.ReLU(),            \n",
        "            nn.Linear(nn_dim, num_labels)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, token_ids, attention_masks, kge):\n",
        "        _, bert_output = self.bert(token_ids, attention_mask=attention_masks, output_all_encoded_layers=False)\n",
        "        drop_output = self.dropout(bert_output)\n",
        "        bert_pool_output = self.bert_pooled(drop_output)\n",
        "        concat = torch.cat((bert_pool_output, kge), dim=1)\n",
        "        nn_output = self.nn(concat)\n",
        "        prob = self.softmax(nn_output)\n",
        "\n",
        "        return prob\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRhRouCQFjhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 datasets are used for the KGE Models. Select the dataset on which the above model can be used\n",
        "dataset = 1\n",
        "\n",
        "if dataset == 1:\n",
        "  df = pd.read_csv(\"movie_genre5.csv\")\n",
        "elif dataset == 2:\n",
        "  df = pd.read_csv(\"med_500.csv\")\n",
        "\n",
        "\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI1wpFd4ysh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing the datasets, setting the number of labels as required and obtaining the metadata needed for getting the KG embeddings \n",
        "if dataset == 1:\n",
        "  df.columns = [\"Title\",\"Director\",\"Genre\",\"Text\",\"Labels\"]\n",
        "  num_labels = 5\n",
        "  file = 'director.pickle'\n",
        "  kg = df.Director.values\n",
        "elif dataset == 2:\n",
        "  df.columns = [\"Drug\",\"Condition\",\"Text\",\"Labels\"]\n",
        "  num_labels = 10\n",
        "  file = 'drug.pickle'\n",
        "  kg = df.Drug.values\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiN7EXzu3OD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Getting the labels and text from the dataframe\n",
        "labels = df.Labels.values\n",
        "sentences = df.Text.values\n",
        "\n",
        "# Loading the KG embeddings from the pickle file\n",
        "with open(file, 'rb') as handle:\n",
        "    kg_dict = pickle.load(handle)\n",
        "\n",
        "# Loading the KG embeddings into a list\n",
        "kge_list = []\n",
        "for entity in kg:\n",
        "  try:\n",
        "    kge_list.append(kg_dict[entity])\n",
        "  except KeyError:\n",
        "    print(entity)\n",
        "\n",
        "# Using the BertTokenizer for tokenizing the words for getting the word embeddings\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# List to hold the token ids\n",
        "token_ids = []\n",
        "\n",
        "# List to hold the attention masks\n",
        "attention_masks = []\n",
        "\n",
        "\n",
        "# Converting each sentence to their respective embeddings and storing them in the list\n",
        "for sent in sentences:\n",
        "\n",
        "    # The encode_plus function adds special tokens which is needed by the BERT for classification\n",
        "    encoded= tokenizer.encode_plus(sent, add_special_tokens = True,  max_length = 512, pad_to_max_length = True, \n",
        "                                         return_attention_mask = True, return_tensors = 'pt', truncation = True)\n",
        "        \n",
        "    token_ids.append(encoded['input_ids'])\n",
        "    \n",
        "    attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "# Converting the tokens, attention masks, labels and kg embeddings to torch tensors\n",
        "token_ids = torch.cat(token_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "kge = torch.tensor(kge_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qux_yjsIJvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the number of epochs and the seed for obtaining the same results \n",
        "epochs = 4\n",
        "seed_val = 1994\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed(seed_val)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# This wrapper function\n",
        "def format_time(time_elapsed):\n",
        "    time_elapsed_rounded = int(round((time_elapsed)))\n",
        "    return str(datetime.timedelta(seconds=time_elapsed_rounded))\n",
        "\n",
        "# Setting the calculations result to 2 decimal points\n",
        "pd.set_option('precision', 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD3I204bETnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kge_model = 1\n",
        "# Using the KFold module for cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "# Using a 4-fold cross validation\n",
        "kf = KFold(n_splits=4)\n",
        "# List to store accuracies\n",
        "accuracies = []\n",
        "# List to hold all the precited labels \n",
        "pred_list = []\n",
        "# List to hold all the true labels\n",
        "true_list = []\n",
        "\n",
        "# Performing 4-fold cross validation\n",
        "for train_index, test_index in kf.split(token_ids):\n",
        "  # Variable to hold the number of correctly predicted sentences\n",
        "  correct = 0\n",
        "\n",
        "  # Tokens, Attention masks, labels and kg embeddings for training set\n",
        "  token_ids_train = token_ids[train_index]\n",
        "  attention_masks_train = attention_masks[train_index]\n",
        "  labels_train = labels[train_index]\n",
        "  kge_train = kge[train_index]\n",
        "\n",
        "  # Tokens, Attention masks, labels and kg embeddings for test set\n",
        "  token_ids_test = token_ids[test_index]\n",
        "  attention_masks_test = attention_masks[test_index]\n",
        "  labels_test = labels[test_index] \n",
        "  kge_test = kge[test_index]\n",
        "\n",
        "  # Converting the tensors to Tensor Dataset\n",
        "  dataset_train = TensorDataset(token_ids_train, attention_masks_train, labels_train, kge_train)\n",
        "  test_data = TensorDataset(token_ids_test, attention_masks_test, labels_test, kge_test)\n",
        "\n",
        "  # Setting the batch size to 4\n",
        "  batch_size = 4\n",
        "\n",
        "  # Test and Training Dataloader needed for parallel processing of the sentences\n",
        "  training_dataloader = DataLoader(dataset_train, sampler = SequentialSampler(dataset_train), batch_size = batch_size )\n",
        "  test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)\n",
        "\n",
        "  # Creating model instance based on the model selected\n",
        "  if kge_model == 1:\n",
        "    model = BERTwithKGE1(num_labels=num_labels)\n",
        "  elif kge_model == 2:\n",
        "    model = BERTwithKGE2(num_labels=num_labels)\n",
        "  elif kge_model == 3:\n",
        "    model = BERTwithKGE3(num_labels=num_labels)\n",
        "\n",
        "\n",
        "  # Setting the model to run on the device\n",
        "  model.to(device)\n",
        "\n",
        "  # Creating an instance of the optimizer with the parameters suggested in the BERT paper\n",
        "  optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
        "\n",
        "  # Setting the total number of steps \n",
        "  total_no_steps = len(training_dataloader) * epochs\n",
        "\n",
        "  # Creating an instance of the scheduler with the parameters suggested in the BERT paper\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_no_steps)\n",
        "\n",
        "\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  # Training the model with 4 epochs\n",
        "  for epoch_i in range(0, epochs):\n",
        "\n",
        "    print('= Epoch {:} / {:} ='.format(epoch_i + 1, epochs))\n",
        "    print('Training the classifier')\n",
        "    t = time.time()\n",
        "\n",
        "    training_loss = 0\n",
        "\n",
        "    # Setting the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(training_dataloader):\n",
        "\n",
        "        if step % 50 == 0 and not step == 0:\n",
        " \n",
        "            time_elapsed = format_time(time.time() - t)\n",
        "            \n",
        "            print('  Batch {:>5,}  of  {:>5,}.      Time Elapsed: {:}.'.format(step, len(training_dataloader), time_elapsed))\n",
        "\n",
        "        # Get the token ids, attention masks, labels and kg embeddings per batch\n",
        "        b_token_ids = batch[0].to(device)\n",
        "        b_attention_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_kge = batch[3].to(device)\n",
        "\n",
        "        # Setting the gradients to zero each time so that previously set gradients are cleared\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Calling the forward function of the model for going through the forward pass of the neural network\n",
        "        prob = model(b_token_ids, \n",
        "                    attention_masks=b_attention_mask,\n",
        "                     kge = b_kge)\n",
        "\n",
        "        # Using the cross entropy loss function for calculating the loss\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        batch_loss = loss_func(prob, b_labels)\n",
        "        training_loss += batch_loss.item()\n",
        "\n",
        "        model.zero_grad()\n",
        "        # Backpropagating after calculating the loss and fine tuning the gradients\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    # Calculating the different stats for display purpose\n",
        "    avg_train_loss = training_loss / len(training_dataloader)            \n",
        "    train_time = format_time(time.time() - t)\n",
        "\n",
        "    print(\"  Average Training Loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training Epoch Time: {:}\".format(train_time))\n",
        "    \n",
        "    training_stats.append(\n",
        "        {\n",
        "            'Epoch': epoch_i + 1,\n",
        "            'Loss during Training': avg_train_loss,\n",
        "            'Time taken for Training': train_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "      \n",
        "  # Creating a dataframe to hold the different stats\n",
        "  stats_df = pd.DataFrame(data=training_stats)\n",
        "\n",
        "  stats_df = stats_df.set_index('Epoch')\n",
        "\n",
        "  print(stats_df)\n",
        "\n",
        "  # Putting the model into evaluation mode \n",
        "  model.eval()\n",
        "\n",
        "  # Lists to hold the predictions and true labels in each of testing set of the cross validation\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  # Predictions of the model\n",
        "  for batch in test_dataloader:\n",
        "\n",
        "    # Convert the inputs to the format needed for the device used\n",
        "    batches = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Get the token ids, attention masks, labels and kg embeddings for each batch\n",
        "    b_token_ids, b_attention_mask, b_labels, b_kge = batches\n",
        "    \n",
        "    # Gradients are not needed as only feedforward needs to be called for the model\n",
        "    with torch.no_grad():\n",
        "        logits = model(b_token_ids, attention_masks=b_attention_mask, kge=b_kge)\n",
        "\n",
        "    # Convert the logits and label ids to the cpu format\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Append predictions and actual labels to their respective lists\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  # Check the number of predictions that are true\n",
        "  for i in range(len(true_labels)):\n",
        "  \n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "    correct += (pred_labels_i == true_labels[i]).sum()  \n",
        "\n",
        "    pred_list += list(pred_labels_i)\n",
        "    true_list += list(true_labels[i]) \n",
        "\n",
        "  # Calculate the accuracy \n",
        "  accuracy = 100 * correct / (len(true_labels)*batch_size)\n",
        "\n",
        "  print(accuracy)\n",
        "  \n",
        "  # Append all the accuracies of the KFold \n",
        "  accuracies.append(accuracy)\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV56SbD33Frn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_list,pred_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiM5FBqAFInB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# K-Fold Accuracy\n",
        "final_accuracy = sum(accuracies)/4\n",
        "final_accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}